# LLM Serving API (FastAPI + HuggingFace + Docker)
This project demonstrates how to build a production-ready LLM inference API using:

ğŸ§  HuggingFace Transformers for text generation

âš¡ FastAPI for serving and REST interface

ğŸ“ˆ Prometheus metrics for observability

ğŸ³ Docker for isolated deployment

ğŸ–¥ï¸ CPU-only setup (GPU-ready upgrade available)

ğŸš€ Features
/generate endpoint to perform text generation

/metrics Prometheus-compatible metrics (latency, request count)

Minimal setup â€” runs on CPU or scales to GPU

Easily extendable with larger models, streaming, or batching

ğŸ“¦ Model
Default model: sshleifer/tiny-gpt2

Lightweight GPT-2 variant

CPU-friendly, zero-cost

Intended for pipeline testing â€” not semantic quality

âœ… You can upgrade to larger CPU-compatible models like:

EleutherAI/gpt-neo-125M

tiiuae/falcon-rw-1b
(all hosted on HuggingFace Hub â€” free to use)

ğŸ›  Tech Stack
FastAPI, Transformers, torch, uvicorn, prometheus_client

Dockerized for reproducibility

ğŸ“ˆ Example Usage
Generate text:
curl "http://localhost:8000/generate?prompt=What+is+AI&max_tokens=64"

Metrics:
http://localhost:8000/metrics

ğŸ§  Future Extensions
Replace model with quantized Mistral or LLaMA

Add OpenTelemetry for request tracing

Add request batching or streaming

Serve via Triton Inference Server or BentoML
